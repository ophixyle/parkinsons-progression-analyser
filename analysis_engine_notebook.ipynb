{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39048bcb-b680-4768-bc2b-461deb7c387b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Analyzer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\oness\\AppData\\Roaming\\Python\\Python313\\site-packages\\scipy\\interpolate\\_interpolate.py:497: RuntimeWarning: invalid value encountered in divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "C:\\Users\\oness\\AppData\\Local\\Temp\\ipykernel_9016\\180679084.py:75: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  rates_df = self.df.groupby('subject#').apply(lambda x: (x['total_UPDRS'].max() - x['total_UPDRS'].min()) / (x['test_time'].max() - x['test_time'].min()) if (x['test_time'].max() - x['test_time'].min()) > 0 else 0).reset_index(name='rate')\n",
      "C:\\Users\\oness\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\oness\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\oness\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\oness\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\oness\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\oness\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\oness\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\oness\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\oness\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\oness\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\oness\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\oness\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\oness\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\oness\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\oness\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\oness\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\oness\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\oness\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Analyzer is ready.\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'cohort_weights' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnboundLocalError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 185\u001b[39m\n\u001b[32m    182\u001b[39m         plot_xai_warping_path(results) \u001b[38;5;66;03m# Call the new XAI plot function\u001b[39;00m\n\u001b[32m    184\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m'\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m     \u001b[43mrun_demonstration\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 167\u001b[39m, in \u001b[36mrun_demonstration\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    165\u001b[39m analyzer = ParkinsonsAnalyzer(CONFIG)\n\u001b[32m    166\u001b[39m test_patient_data = [\u001b[32m22\u001b[39m, \u001b[32m25\u001b[39m, \u001b[32m26\u001b[39m, \u001b[32m29\u001b[39m, \u001b[32m32\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m results = \u001b[43manalyzer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_patient_series\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest_patient_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatient_age\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m65\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatient_sex\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33merror\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m results:\n\u001b[32m    170\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mAnalysis Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults[\u001b[33m'\u001b[39m\u001b[33merror\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 115\u001b[39m, in \u001b[36mParkinsonsAnalyzer.predict\u001b[39m\u001b[34m(self, new_patient_series, patient_age, patient_sex)\u001b[39m\n\u001b[32m    112\u001b[39m top_matches = \u001b[38;5;28mself\u001b[39m._find_top_k_matches(new_patient_series, patient_age, patient_sex)\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m top_matches: \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33merror\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mCould not find any suitable matches for the given patient context.\u001b[39m\u001b[33m\"\u001b[39m}\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m forecast, cohort, confidence = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_weighted_forecast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtop_matches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_patient_series\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    116\u001b[39m best_match = top_matches[\u001b[32m0\u001b[39m]\n\u001b[32m    118\u001b[39m \u001b[38;5;66;03m# --- XAI LAYER DATA GENERATION ---\u001b[39;00m\n\u001b[32m    119\u001b[39m \u001b[38;5;66;03m# Calculate the warping path for the single best match to explain the connection.\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 104\u001b[39m, in \u001b[36mParkinsonsAnalyzer._generate_weighted_forecast\u001b[39m\u001b[34m(self, top_matches, input_series_len)\u001b[39m\n\u001b[32m    102\u001b[39m normalized_weights = weights / np.sum(weights)\n\u001b[32m    103\u001b[39m forecast = np.average(future_trajectories, axis=\u001b[32m0\u001b[39m, weights=normalized_weights)\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m cohort_weights = {match[\u001b[33m'\u001b[39m\u001b[33mcohort\u001b[39m\u001b[33m'\u001b[39m]: \u001b[43mcohort_weights\u001b[49m.get(match[\u001b[33m'\u001b[39m\u001b[33mcohort\u001b[39m\u001b[33m'\u001b[39m], \u001b[32m0\u001b[39m) + weight \u001b[38;5;28;01mfor\u001b[39;00m match, weight \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(top_matches, normalized_weights)}\n\u001b[32m    105\u001b[39m predicted_cohort = \u001b[38;5;28mmax\u001b[39m(cohort_weights, key=cohort_weights.get)\n\u001b[32m    106\u001b[39m confidence = (cohort_weights[predicted_cohort] / \u001b[38;5;28msum\u001b[39m(cohort_weights.values())) * \u001b[32m100\u001b[39m\n",
      "\u001b[31mUnboundLocalError\u001b[39m: cannot access local variable 'cohort_weights' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# PARKINSON'S SYMPTOM PROGRESSION ANALYZER - REFACTORED ENGINE\n",
    "# Author: Onessa Crispeyn\n",
    "#\n",
    "# Description:\n",
    "# This refactored version encapsulates the entire analysis pipeline within a\n",
    "# reusable `ParkinsonsAnalyzer` class. It separates the core logic from the\n",
    "# demonstration, making the code cleaner, more modular, and easier to maintain\n",
    "# or integrate into other applications like a web server.\n",
    "# ==============================================================================\n",
    "\n",
    "# --- Part 1: Imports and Configuration ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.interpolate import interp1d\n",
    "from dtaidistance import dtw\n",
    "\n",
    "# Centralized configuration for easy tweaking of model parameters\n",
    "CONFIG = {\n",
    "    \"data_path\": './parkinsons_updrs.data',\n",
    "    \"interpolation_points\": 100,\n",
    "    \"n_clusters\": 2,\n",
    "    \"k_matches\": 10,\n",
    "    \"age_window\": 5,\n",
    "    \"random_state\": 42\n",
    "}\n",
    "\n",
    "class ParkinsonsAnalyzer:\n",
    "    \"\"\"A class to encapsulate the entire Parkinson's analysis pipeline.\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        \"\"\"\n",
    "        Initializes the analyzer by loading data, training models, and preparing assets.\n",
    "        This heavy lifting is done only once when the class is instantiated.\n",
    "        \"\"\"\n",
    "        print(\"Initializing Analyzer...\")\n",
    "        self.config = config\n",
    "        self.df = self._load_and_preprocess_data(config[\"data_path\"])\n",
    "        self.all_trajectories = self._standardize_all_trajectories(self.df)\n",
    "        \n",
    "        # Train the cohort discovery model\n",
    "        self._discover_cohorts()\n",
    "        \n",
    "        # Prepare a global scaler for consistent scaling of new data\n",
    "        self.global_scaler = StandardScaler().fit(self.df[['total_UPDRS']])\n",
    "        print(\"✅ Analyzer is ready.\")\n",
    "\n",
    "    def _load_and_preprocess_data(self, path):\n",
    "        \"\"\"Loads and cleans the raw Parkinson's data.\"\"\"\n",
    "        column_names = [\n",
    "            'subject#', 'age', 'sex', 'test_time', 'motor_UPDRS', 'total_UPDRS', 'Jitter(%)', 'Jitter(Abs)',\n",
    "            'Jitter:RAP', 'Jitter:PPQ5', 'Jitter:DDP', 'Shimmer', 'Shimmer(dB)', 'Shimmer:APQ3', 'Shimmer:APQ5',\n",
    "            'Shimmer:APQ11', 'Shimmer:DDA', 'NHR', 'HNR', 'RPDE', 'DFA', 'PPE'\n",
    "        ]\n",
    "        df = pd.read_csv(path, names=column_names)\n",
    "        df = df[['subject#', 'age', 'sex', 'test_time', 'total_UPDRS']]\n",
    "        df[['total_UPDRS', 'test_time', 'age']] = df[['total_UPDRS', 'test_time', 'age']].apply(pd.to_numeric, errors='coerce')\n",
    "        df.dropna(inplace=True)\n",
    "        return df\n",
    "\n",
    "    def _standardize_all_trajectories(self, df):\n",
    "        \"\"\"Interpolates all patient trajectories to a standard length.\"\"\"\n",
    "        def normalize_and_interpolate(patient_df):\n",
    "            if len(patient_df) < 2: return None\n",
    "            min_time, max_time = patient_df['test_time'].min(), patient_df['test_time'].max()\n",
    "            if max_time == min_time: return None\n",
    "            time_normalized = (patient_df['test_time'] - min_time) / (max_time - min_time)\n",
    "            interp_func = interp1d(time_normalized, patient_df['total_UPDRS'], kind='linear', fill_value=\"extrapolate\")\n",
    "            return interp_func(np.linspace(0, 1, self.config[\"interpolation_points\"]))\n",
    "\n",
    "        all_trajectories = {sid: normalize_and_interpolate(pdf) for sid, pdf in df.groupby('subject#')}\n",
    "        return {sid: traj for sid, traj in all_trajectories.items() if traj is not None and not np.isnan(traj).any()}\n",
    "\n",
    "    def _discover_cohorts(self):\n",
    "        \"\"\"Uses K-Means clustering to discover and label patient cohorts.\"\"\"\n",
    "        subject_ids = list(self.all_trajectories.keys())\n",
    "        trajectory_matrix = np.array(list(self.all_trajectories.values()))\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        scaled_trajectories = scaler.fit_transform(trajectory_matrix)\n",
    "        \n",
    "        kmeans = KMeans(n_clusters=self.config[\"n_clusters\"], random_state=self.config[\"random_state\"], n_init=10)\n",
    "        clusters = kmeans.fit_predict(scaled_trajectories)\n",
    "        \n",
    "        cohort_df = pd.DataFrame({'subject#': subject_ids, 'cluster': clusters})\n",
    "        rates_df = self.df.groupby('subject#').apply(lambda x: (x['total_UPDRS'].max() - x['total_UPDRS'].min()) / (x['test_time'].max() - x['test_time'].min()) if (x['test_time'].max() - x['test_time'].min()) > 0 else 0).reset_index(name='rate')\n",
    "        cohort_df = pd.merge(cohort_df, rates_df, on='subject#')\n",
    "        \n",
    "        cluster_avg_rates = cohort_df.groupby('cluster')['rate'].mean()\n",
    "        fast_cluster_id = cluster_avg_rates.idxmax()\n",
    "        cohort_df['cohort'] = np.where(cohort_df['cluster'] == fast_cluster_id, 'Fast Progressor', 'Slow Progressor')\n",
    "        \n",
    "        # Merge cohort info back into the main dataframe\n",
    "        self.df = pd.merge(self.df, cohort_df[['subject#', 'cohort']], on='subject#')\n",
    "\n",
    "    def _find_top_k_matches(self, new_patient_series, patient_age, patient_sex):\n",
    "        \"\"\"Finds top matches by comparing the input series to historical starting segments.\"\"\"\n",
    "        input_series_scaled = self.global_scaler.transform(np.array(new_patient_series).reshape(-1, 1))\n",
    "        input_len = len(input_series_scaled)\n",
    "\n",
    "        target_df = self.df[self.df['age'].between(patient_age - self.config[\"age_window\"], patient_age + self.config[\"age_window\"]) & (self.df['sex'] == patient_sex)]\n",
    "        if len(target_df['subject#'].unique()) < self.config[\"k_matches\"]:\n",
    "            target_df = self.df # Fallback to all data if context filter is too restrictive\n",
    "\n",
    "        all_matches = []\n",
    "        for sid in target_df['subject#'].unique():\n",
    "            if sid in self.all_trajectories:\n",
    "                historical_traj = self.all_trajectories[sid]\n",
    "                historical_traj_scaled = self.global_scaler.transform(historical_traj.reshape(-1, 1))\n",
    "                comparison_segment = historical_traj_scaled[:input_len]\n",
    "                dist = dtw.distance(input_series_scaled, comparison_segment)\n",
    "                all_matches.append({\n",
    "                    \"match_id\": sid, \"distance\": dist,\n",
    "                    \"cohort\": target_df[target_df['subject#'] == sid]['cohort'].iloc[0],\n",
    "                    \"full_trajectory\": historical_traj\n",
    "                })\n",
    "        \n",
    "        return sorted(all_matches, key=lambda x: x['distance'])[:self.config[\"k_matches\"]]\n",
    "\n",
    "    def _generate_weighted_forecast(self, top_matches, input_series_len):\n",
    "        \"\"\"Generates a forecast from a weighted average of the top matches.\"\"\"\n",
    "        distances = np.array([match['distance'] for match in top_matches])\n",
    "        future_trajectories = np.array([match['full_trajectory'][input_series_len:] for match in top_matches])\n",
    "        weights = 1 / (distances + 1e-9)\n",
    "        normalized_weights = weights / np.sum(weights)\n",
    "        forecast = np.average(future_trajectories, axis=0, weights=normalized_weights)\n",
    "        \n",
    "        cohort_weights = {}\n",
    "        for match, weight in zip(top_matches, normalized_weights):\n",
    "            cohort_weights[match['cohort']] = cohort_weights.get(match['cohort'], 0) + weight\n",
    "        predicted_cohort = max(cohort_weights, key=cohort_weights.get)\n",
    "        confidence = (cohort_weights[predicted_cohort] / sum(cohort_weights.values())) * 100\n",
    "        return forecast, predicted_cohort, confidence\n",
    "\n",
    "    def predict(self, new_patient_series, patient_age, patient_sex):\n",
    "        \"\"\"\n",
    "        The main public method. Takes new patient data and returns a full analysis.\n",
    "        \"\"\"\n",
    "        if len(new_patient_series) < 2:\n",
    "            return {\"error\": \"Input series must have at least 2 data points.\"}\n",
    "            \n",
    "        top_matches = self._find_top_k_matches(new_patient_series, patient_age, patient_sex)\n",
    "        if not top_matches:\n",
    "            return {\"error\": \"Could not find any suitable matches for the given patient context.\"}\n",
    "            \n",
    "        forecast, cohort, confidence = self._generate_weighted_forecast(top_matches, len(new_patient_series))\n",
    "        \n",
    "        return {\n",
    "            \"input_data\": new_patient_series,\n",
    "            \"forecast\": forecast,\n",
    "            \"predicted_cohort\": cohort,\n",
    "            \"confidence\": confidence,\n",
    "            \"best_match\": top_matches[0],\n",
    "            \"top_matches_count\": len(top_matches)\n",
    "        }\n",
    "\n",
    "# --- Part 7: Demonstration (Separated from the core logic) ---\n",
    "\n",
    "def plot_forecast(results):\n",
    "    \"\"\"Visualizes the analysis results.\"\"\"\n",
    "    input_data = results['input_data']\n",
    "    input_len = len(input_data)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(14, 8))\n",
    "    x_axis_full = np.arange(CONFIG[\"interpolation_points\"])\n",
    "    x_axis_input = x_axis_full[:input_len]\n",
    "    x_axis_forecast = x_axis_full[input_len-1:]\n",
    "\n",
    "    ax.plot(x_axis_input, input_data, label='Input Patient Data', color='red', marker='o', markersize=8, linewidth=4, zorder=10)\n",
    "    \n",
    "    stitched_forecast_y = np.concatenate(([input_data[-1]], results['forecast']))\n",
    "    ax.plot(x_axis_forecast, stitched_forecast_y, color='green', linewidth=4, linestyle='--', label='Synthesized Forecast')\n",
    "\n",
    "    best_match_traj = results['best_match']['full_trajectory']\n",
    "    ax.plot(x_axis_full, best_match_traj, label=f'Best Match (Patient #{results[\"best_match\"][\"match_id\"]})', \n",
    "            linestyle=':', alpha=0.8, color='gray', zorder=-1)\n",
    "    \n",
    "    ax.set_title(f\"Forecast: Trajectory aligns with '{results['predicted_cohort']}' Cohort\", fontsize=18)\n",
    "    ax.set_xlabel('Normalized Time Horizon (% of total observation period)', fontsize=12)\n",
    "    ax.set_ylabel('UPDRS Score', fontsize=12)\n",
    "    ax.legend(fontsize=10)\n",
    "    ax.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def run_demonstration():\n",
    "    \"\"\"Example of how to use the ParkinsonsAnalyzer class.\"\"\"\n",
    "    # This only runs once, creating a reusable analyzer instance\n",
    "    analyzer = ParkinsonsAnalyzer(CONFIG)\n",
    "    \n",
    "    # Now we can run predictions as many times as we want\n",
    "    test_patient_data = [22, 25, 26, 29, 32]\n",
    "    results = analyzer.predict(new_patient_series=test_patient_data, patient_age=65, patient_sex=0)\n",
    "\n",
    "    if \"error\" in results:\n",
    "        print(f\"\\nAnalysis Error: {results['error']}\")\n",
    "    else:\n",
    "        print(\"\\n--- Predictive Forecast Report ---\")\n",
    "        print(f\"Input Data: {results['input_data']}\")\n",
    "        print(f\"Context: Age≈65, Sex=Male (found {results['top_matches_count']} matches)\")\n",
    "        print(\"------------------------------------\")\n",
    "        print(f\"Predicted Cohort: {results['predicted_cohort']}\")\n",
    "        print(f\"Prediction Confidence: {results['confidence']:.1f}%\")\n",
    "        print(f\"Closest Single Match: Patient #{results['best_match']['match_id']}\")\n",
    "        print(\"------------------------------------\")\n",
    "        \n",
    "        plot_forecast(results)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run_demonstration()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
